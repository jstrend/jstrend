{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dced0f2a",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75ad7d4",
   "metadata": {},
   "source": [
    "### https://scikit-learn.org/stable/getting_started.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644315fd",
   "metadata": {},
   "source": [
    "### It assumes a very basic working knowledge of machine learning practices (model fitting, predicting, cross-validation, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da0922f",
   "metadata": {},
   "source": [
    "### Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection, model evaluation, and many other utilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfad85b7",
   "metadata": {},
   "source": [
    "## Fitting and predicting: estimator basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec38453",
   "metadata": {},
   "source": [
    "### Scikit-learn provides dozens of built-in machine learning algorithms and models, called estimators. Each estimator can be fitted to some data using its fit method.\n",
    "\n",
    "### Here is a simple example where we fit a RandomForestClassifier to some very basic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b610880d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "X = [[ 1,  2,  3],  # 2 samples, 3 features\n",
    "     [11, 12, 13]]\n",
    "y = [0, 1]  # classes of each sample\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ece572d",
   "metadata": {},
   "source": [
    "### about row(행), column(열)\n",
    "### 가로: 오, 행, 횡, row, horizontal\n",
    "### 세로: 열, 종, column, vertical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca8ee68",
   "metadata": {},
   "source": [
    "### The fit method generally accepts 2 inputs:\n",
    "\n",
    "### The samples matrix (or design matrix) X. The size of X is typically (n_samples, n_features), which means that samples are represented as rows and features are represented as columns.\n",
    "\n",
    "### The target values y which are real numbers for regression(회귀) tasks, or integers for classification (or any other discrete(이산, 분리된) set of values). For unsupervized learning tasks, y does not need to be specified. y is usually 1d array where the i th entry corresponds to the target of the i th sample (row) of X.\n",
    "\n",
    "### Both X and y are usually expected to be numpy arrays or equivalent array-like data types, though some estimators work with other formats such as sparse(부족한, 희박한) matrices(희소행렬: 대부분의 값이 0인 행렬, <-> dense matrices).\n",
    "\n",
    "### Once the estimator is fitted, it can be used for predicting target values of new data. You don’t need to re-train the estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1c0f98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X)  # predict classes of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5c919e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[4, 5, 6], [14, 15, 16]])  # predict classes of new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a322affb",
   "metadata": {},
   "source": [
    "## Transformers and pre-processors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a84bc8",
   "metadata": {},
   "source": [
    "### Machine learning workflows are often composed of different parts. A typical pipeline consists of a pre-processing step that transforms or imputes the data, and a final predictor that predicts target values.\n",
    "### In scikit-learn, pre-processors and transformers follow the same API as the estimator objects (they actually all inherit from the same BaseEstimator class). The transformer objects don’t have a predict method but rather a transform method that outputs a newly transformed sample matrix X:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5cca3d",
   "metadata": {},
   "source": [
    "### scikit-lkearn 데이터 전처리 스케일 조정(스케일러) \n",
    "### Standard Scaler: 평균 0 , 분산 1로 조정, fit, transform, fit_transform 지원\n",
    "### Robust Scaler: 평균과 분산 대신 중간값(median)과 사분위값을 사용, (X - Q2) / (Q3-Q1), [Q2 = 2분위값, Q3 = 3분위값, Q1 = 1분위값]\n",
    "### MinMax Scaler: (X-Xmin)/(Xmax-Xmin), 모든 값이 0~1사이에 존재, 정규화 방법중 원데이터 분포를 유지하면서 정규화하는 방법\n",
    "### Normalizer: 특성벡터의 모든 길이가 1이 되도록 조정(반지름 1인 원에 투영하는 느낌), 특성벡터의 길이는 상관없고, 데이터의 방향이나 각도가 중요할 경우 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5971d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.,  1.],\n",
       "       [ 1., -1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X = [[0, 15],\n",
    "     [1, -10]]\n",
    "# scale data according to computed scaling values\n",
    "StandardScaler().fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fd4947",
   "metadata": {},
   "source": [
    "## Pipelines: chaining pre-processors and estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b83ca5",
   "metadata": {},
   "source": [
    "### Transformers and estimators (predictors) can be combined together into a single unifying object: a Pipeline. The pipeline offers the same API as a regular estimator: it can be fitted and used for prediction with fit and predict. As we will see later, using a pipeline will also prevent you from data leakage, i.e. disclosing some testing data in your training data.\n",
    "\n",
    "### In the following example, we load the Iris dataset, split it into train and test sets, and compute the accuracy score of a pipeline on the test data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59e8444",
   "metadata": {},
   "source": [
    "### 로지스틱 모형 식은 독립 변수가 (-∞,∞)의 어느 숫자이든 상관 없이 종속 변수 또는 결과 값이 항상 범위 [0,1] 사이에 있도록 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7d580e",
   "metadata": {},
   "source": [
    "### (e^x) / (1+e^x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0079d580",
   "metadata": {},
   "source": [
    "### 파이프라인 (Pipeline) 은 전처리의 단계인 모델 생성, 학습 등을 포함하는 여러 단계의 머신러닝 프로세스를 한 번에 처리할 수 있는 클래스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1a7d25",
   "metadata": {},
   "source": [
    "### random_state=0는 random seed의 선택값, 즉 동일한 랜덤씨드를 사용하는 번호"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "000bfd9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('logisticregression', LogisticRegression())])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "# create a pipeline object\n",
    "pipe = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression()\n",
    ")\n",
    "# load the iris dataset and split it into train and test sets\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "# fit the whole pipeline\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0e91f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can now use it like any other estimator\n",
    "accuracy_score(pipe.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5829f9b6",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7e4911",
   "metadata": {},
   "source": [
    "### Fitting a model to some data does not entail(수반하다) that it will predict well on unseen data. This needs to be directly evaluated. We have just seen the train_test_split helper that splits a dataset into train and test sets, but scikit-learn provides many other tools for model evaluation, in particular for cross-validation(evaluating estimator performance).\n",
    "\n",
    "### We here briefly show how to perform a 5-fold cross-validation procedure, using the cross_validate helper. Note that it is also possible to manually iterate over the folds, use different data splitting strategies, and use custom scoring functions. Please refer to our User Guide for more details:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c940e92",
   "metadata": {},
   "source": [
    "###  make_regression은 datasets서브 패키지에  회귀 분석 시험용 가상 데이터를 생성하는 명령어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d10b1485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "X, y = make_regression(n_samples=1000, random_state=0)\n",
    "lr = LinearRegression()\n",
    "result = cross_validate(lr, X, y)  # defaults to 5-fold CV(Cross_Validate)\n",
    "result['test_score']  # r_squared score is high because dataset is easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5895db6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.00400114, 0.00400066, 0.00300074, 0.00300074, 0.0040009 ]),\n",
       " 'score_time': array([0.0010004, 0.       , 0.       , 0.       , 0.       ]),\n",
       " 'test_score': array([1., 1., 1., 1., 1.])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a7f524",
   "metadata": {},
   "source": [
    "## Automatic parameter searches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a793951",
   "metadata": {},
   "source": [
    "### All estimators have parameters (often called hyper-parameters in the literature) that can be tuned. The generalization power of an estimator often critically depends on a few parameters. For example a RandomForestRegressor has a n_estimators parameter that determines the number of trees in the forest, and a max_depth parameter that determines the maximum depth of each tree. Quite often, it is not clear what the exact values of these parameters should be since they depend on the data at hand.\n",
    "### Scikit-learn provides tools to automatically find the best parameter combinations (via cross-validation). In the following example, we randomly search over the parameter space of a random forest with a RandomizedSearchCV object. When the search is over, the RandomizedSearchCV behaves as a RandomForestRegressor that has been fitted with the best set of parameters. Read more in the User Guide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fddfaca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=RandomForestRegressor(random_state=0), n_iter=5,\n",
       "                   param_distributions={'max_depth': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001B8B6C665E0>,\n",
       "                                        'n_estimators': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001B8B1AD49D0>},\n",
       "                   random_state=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import randint\n",
    "X, y = fetch_california_housing(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "# define the parameter space that will be searched over\n",
    "param_distributions = {'n_estimators': randint(1, 5),\n",
    "                       'max_depth': randint(5, 10)}\n",
    "# now create a searchCV object and fit it to the data\n",
    "search = RandomizedSearchCV(estimator=RandomForestRegressor(random_state=0),\n",
    "                            n_iter=5,\n",
    "                            param_distributions=param_distributions,\n",
    "                            random_state=0)\n",
    "search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "810f1135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 9, 'n_estimators': 4}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b57f017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.735363411343253"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the search object now acts like a normal random forest estimator\n",
    "# with max_depth=9 and n_estimators=4\n",
    "search.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca687102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
